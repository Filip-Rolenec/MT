{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADP algorihtm \n",
    "- The goal of this file is to first learn how does the ADP work on a simple example and then use it to solve the problem of gas power plant valuation. \n",
    "\n",
    "- First, I will setup a simple example of three states and two actions. Each of the two actions changes the probability distributions of results and \"costs\" some reward. \n",
    "    - I will compute the optimal strategy for this example with real dynamic programming and then with the approximative dynamic programming. \n",
    "    - I will make heuristic strategies as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three states two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Users/filiprolenec/Desktop/MT/MTpython/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simple_example.strategy as s\n",
    "from simple_example.simulation import run_simulation\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from simple_example.setup import ProblemSetup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from simple_example.state import get_new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Dynamic programming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_example.dp_algorithm import classic_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_vf = [0,0,0]\n",
    "problem_setup = ProblemSetup()\n",
    "prob_matrix = problem_setup.prob_matrix\n",
    "reward_matrix = problem_setup.reward_matrix\n",
    "time_epochs = problem_setup.time_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({9: {1: 2, 2: 1, 3: 2},\n",
       "  8: {1: 2, 2: 1, 3: 2},\n",
       "  7: {1: 2, 2: 1, 3: 2},\n",
       "  6: {1: 2, 2: 1, 3: 2},\n",
       "  5: {1: 2, 2: 1, 3: 2},\n",
       "  4: {1: 2, 2: 1, 3: 2},\n",
       "  3: {1: 2, 2: 1, 3: 2},\n",
       "  2: {1: 2, 2: 1, 3: 2},\n",
       "  1: {1: 2, 2: 1, 3: 2},\n",
       "  0: {1: 2, 2: 1, 3: 2}},\n",
       " {9: [6.3999999999999995, 13.8, 14.399999999999999],\n",
       "  8: [19.88, 26.48, 26.16],\n",
       "  7: [31.995999999999995, 38.768, 38.804],\n",
       "  6: [44.516, 51.2352, 51.1472],\n",
       "  5: [56.90168, 63.63856, 63.593039999999995],\n",
       "  4: [69.333008, 76.063872, 76.00384],\n",
       "  3: [81.7487632, 88.48168, 88.4266032],\n",
       "  2: [94.16983456, 100.90205055999999, 100.84528191999999],\n",
       "  1: [106.58909091199999, 113.321546176, 113.26535516799999],\n",
       "  0: [119.00896694399998, 125.7413405184, 125.6849522944]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classic_dp(horizon_vf, problem_setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [s.heuristic_strategy_0, \n",
    "              s.heuristic_strategy_1, \n",
    "              s.heuristic_strategy_2, \n",
    "              s.optimal_strategy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "initial_state = 0\n",
    "\n",
    "for strategy in strategies: \n",
    "    strategy_results = {}\n",
    "    for i in range(10000): \n",
    "        strategy_results[i] = run_simulation(strategy, initial_state, problem_setup)\n",
    "            \n",
    "    all_results[strategy.__name__] = strategy_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x123818970>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAirElEQVR4nO3de3xU9bnv8c/DvRo0gJfjBs4JtSiXXLkKASuigBzkUoWiWAioVIWzUbe6Acu2x244dkvFUlGrBZRuAbUURURFqAh4AQONEAIIlCiJFBEKQgMY4u/8MStxgMl9MjPJ+r5fr3llrd/6rbWeWZN5svJba54x5xwiIuIP9aIdgIiIRI6SvoiIjyjpi4j4iJK+iIiPKOmLiPhIg2gHUJaLLrrIJSQkRDsMEZFaZdOmTV875y4OtSymk35CQgKZmZnRDkNEpFYxs89LW6bhHRERH1HSFxHxESV9EREfiekx/VAKCwvJy8vj5MmT0Q5FYlSTJk1o1aoVDRs2jHYoIjGn1iX9vLw8mjZtSkJCAmYW7XAkxjjnOHToEHl5ebRp0yba4YjEnFo3vHPy5ElatGihhC8hmRktWrTQf4Iipah1SR9Qwpcy6fdDpHS1MumLiEjV1Lox/bPd/sInYd3e3IyuYd2eiEgsqfVJPxpyc3MZNGgQ2dnZYd92z549+fDDD0tdPmPGDKZOnVrh/hW1Zs0aGjVqRM+ePSu1XlZWFl9++SUDBw6sdgwV8fbbbzNp0iSKioq44447mDx5ckT2WxkTV0+MyH6e6vtURPYjdYuGd2LE6dOnAcpN4DNmzDhjPhwJHwJJv7RtFccWSlZWFitWrAhLDOUpKipiwoQJvPXWW+Tk5LBo0SJycnIism+RukJJv4qKioq488476dixI/369ePEiRPs2bOHAQMG0LlzZ3r37s2OHTsAyMjI4E9/+lPJunFxcUAg0fbu3ZvBgwfToUOHM5bt37+fq6++mtTUVBITE1m3bh2TJ0/mxIkTpKamMmrUqDP6A/z6178mKSmJlJSUMs+AZ8+eTYcOHUhOTmbkyJHk5uby7LPPMmvWLFJTU1m3bh0ZGRncdddddO/enYceeoiNGzfSo0cP0tLS6NmzJzt37uTbb7/lP/7jP3j55ZdJTU3l5Zdf5p///Cfjxo2jW7dupKWl8frrrwNQUFDAiBEj6NChA8OGDaN79+5kZmYyb9487r333pLYnn/+ee67776QcW/cuJEf/ehH/PCHP6RRo0aMHDmyZPsiUjEa3qmiXbt2sWjRIp5//nlGjBjBkiVLmD9/Ps8++yxt27Zlw4YN3HPPPfzlL38pczubN28mOzv7nHvKFy5cSP/+/Xn44YcpKiqioKCA3r1789RTT5GVlXXOdt566y1ef/11NmzYwHnnncfhw4dL3edjjz3G3r17ady4MUeOHCE+Pp677rqLuLg4HnjgAQDmzp1LXl4eH374IfXr1+ebb75h3bp1NGjQgFWrVjF16lSWLFnCo48+SmZmJk89FRhqmDp1Ktdeey3z5s3jyJEjdOvWjeuuu45nnnmGZs2akZOTQ3Z2NqmpqQCMGDGC6dOn8/jjj9OwYUPmz5/P73//+5Bx5+fn07p165L5Vq1asWHDhjKPr4icSUm/itq0aVOSuDp37kxubi4ffvghw4cPL+lz6tSpcrfTrVu3kB8i6tq1K+PGjaOwsJChQ4eW7Ks0q1atYuzYsZx33nkANG/evNS+ycnJjBo1iqFDhzJ06NBS+w0fPpz69esDcPToUcaMGcOuXbswMwoLC0Ous3LlSpYtW8bMmTOBwOcqvvjiC9avX8+kSZMASExMJDk5GQj8p3LttdeyfPly2rdvT2FhIUlJSWU+VxGpOiX9KmrcuHHJdP369Tlw4ADx8fEhz8IbNGjAd999B8B3333Ht99+W7Ls/PPPD7n9q6++mrVr1/Lmm2+SkZHB/fffz+jRo8MS+5tvvsnatWt54403mD59Olu3bg3ZLzi2adOm0adPH5YuXUpubi7XXHNNyHWccyxZsoQrr7yywvHccccdzJgxg3bt2jF27NhS+7Vs2ZJ9+/aVzOfl5dGyZcsK7ydSF1hFYlmtT/qxcovlBRdcQJs2bXj11VcZPnw4zjm2bNlCSkoKCQkJbNq0iREjRrBs2bJSz5KDff7557Rq1Yo777yTU6dOsXnzZkaPHk3Dhg0pLCw8p67M9ddfz6OPPsqoUaNKhndCne1/99137Nu3jz59+tCrVy8WL17M8ePHadq0Kd98802p8Rw9erQkwb7wwgsl7U2bNuXYsWMl8/379+d3v/sdv/vd7zAz/vrXv5KWlkZ6ejqvvPIKffr0IScn54w/NN27d2ffvn1s3ryZLVu2lBpD165d2bVrF3v37qVly5YsXryYhQsXlnssReR7upAbRi+99BJz584lJSWFjh07llxkvPPOO3n//fdJSUnho48+KvXsPtiaNWtISUkhLS2Nl19+uWRoZPz48SXDM8EGDBjA4MGD6dKlC6mpqSXDK2crKiritttuIykpibS0NP71X/+V+Ph4brzxRpYuXVpyIfdsDz30EFOmTCEtLe2Mu3mKk3jxhdxp06ZRWFhIcnIyHTt2ZNq0aQDcc889HDx4kA4dOvCLX/yCjh07cuGFF5ZsZ8SIEaSnp9OsWbNSj0mDBg146qmn6N+/P+3bt2fEiBF07Nix3GMpIt8z51y0YyhVly5d3NnfnLV9+3bat28fpYikqoqKiigsLKRJkybs2bOH6667jp07d9KoUSMABg0axH333Uffvn3Dsr9Qvyd1bXhH9+lLacxsk3OuS6hltX54R2qHgoIC+vTpQ2FhIc45nn76aRo1alRyh09KSkrYEr6IlE5Jvw6bMGECH3zwwRltkyZNKvNiaU1p2rRpyO87jo+P57PPPjuj7dChQyH/AKxevZoWLVrUWIx1QbjLkpQmVq6lSeUp6ddhc+bMiXYIVdKiRYuQd0GJSPXpQq6IiI8o6YuI+Ei5Sd/MWpvZe2aWY2bbzGyS1/5LM8s3syzvMTBonSlmttvMdppZ/6D2AV7bbjOLvfKIIiJ1XEXG9E8D/+ac22xmTYFNZvaut2yWc+6MG8LNrAMwEugI/Auwysyu8BbPAa4H8oBPzGyZc656ZRIX/rRaq5/j1pfDuz0RkRhS7pm+c26/c26zN30M2A6U9dn3IcBi59wp59xeYDfQzXvsds79zTn3LbDY61vr5ObmkpiYWCPbLq+e/dmllStb/740ZZVWLkskSysDjBs3jksuuaTGjr9IXVepMX0zSwDSgOLShhPNbIuZzTOz4o9StgT2Ba2W57WV1i6onn5FZWRk8Pbbb0dsfyJ1TYWTvpnFAUuAe51z3wDPAJcDqcB+4DfhCMjMxptZppllHjx4MBybrBGqpx/5evoQKERXVgVRESlbhe7TN7OGBBL+S865PwM45w4ELX8eWO7N5gOtg1Zv5bVRRnsJ59xzwHMQKMNQoWcRBaqnH/l6+iJSfeUmfTMzYC6w3Tn3RFD7Zc65/d7sMKD4C2OXAQvN7AkCF3LbAhsBA9qaWRsCyX4kcGu4nkikqZ6+6umL1EYVOdNPB34GbDWzLK9tKnCLmaUCDsgFfg7gnNtmZq8AOQTu/JngnCsCMLOJwDtAfWCec25b2J5JhKme/jUh16nJevoiUn3lJn3n3HoCZ+lnK/XqnXNuOjA9RPuKstarkhi5xVL19ANqsp6+iFSfPpEbRqqnX7P19AFuueUWevTowc6dO2nVqhVz584t91iKyPdUT18iQvX0wy9UPX1V2RRQPX2JAaqnLxIblPTrsJirp//OWddfDu0hHvjso7dK5gEOHf4HfX9y7kXr1X9eQIvmZQ//0OLy6gcrUocp6ddhtbaefvNmZK15I9phSAzad9fdVV639bPPhDGS2ksXckVEfERJX0TER5T0RUR8pNaP6Yf7NrxQt8GJiNQVtT7pR0Nubi6DBg0iOzu7/M6V1LNnzzLLJc+YMYOpU6dWuH9FrVmzhkaNGlW6Pn9WVhZffvklAwcOLL9zNe3L/5LR9zzIgYNfY2aMHz2SST/PqPH9hsun+46EdXuRuidf6hYN78QI1dMvX4P6DfjNo1PI+fAdPn77T8yZ+9/k7NwVkX2L1BVK+lWkevpVqKd/+/+hQ8/+DBt9N9373UTmX7cy76VXuffh/yyJ7fkFi7kvaD7YZf/jEjqlBL4xq2nTONpfcTn5+w+E7CsioWl4p4pUT78K9fQvvJCcD98he/tnpF5zIwAjhgxk+qynefyX/x6op79oCb//TeikHyz3izz+ujWH7p1Tyu0rIt9T0q8i1dOvQj39jJsBSGx/BckdAqWX4+LO59pePVi+8j3aX3E5hYWnSepQdlnm48f/yU0ZE3hy+i+4oGnTMvuKyJk0vFNFZ9fTP3z4cEk9/eLH9u3bgerV02/ZsiUZGRksWLAgbLG/+eabTJgwgc2bN9O1a9dSx+xD1dPPzs7mjTfe4OTJkyHXKa6nX3wMvvjii3IL5N3xsxG8sGgJ8xcuYeytN5XZt7CwkJvGTmDUzYP5yaD+5TxTETlbrT/Tj5VbLFVPP6DMevqvv0Wf3j3I2bmLrds/K1mne+dU9uXvZ/OWbWxZ+2apMTjnuH3SFNpf8SPuv+f2co+hiJxLZ/phpHr65dTTP3SIDj3784sZs+jYri0XXvD90MyIoQNJ79aZZvEXnrPvYh9s2MQfX3mNv6z7iNRrbiT1mhtZ8e6aco+liHxP9fQlIoqKiij8+06aNGnMnr2fc91NY9j58crv6+nfcif33T2WvldX7nMC5/CqbMZiPf1w36d/BZPCur3KiFY9fRVcqxjV05eoKygooM///imFp08H6un/1/8N1NM/+g3drv8JKYntqp/wRaRcSvp1WMzV01/92jnt8RdewGcbV53RVq16+iJSJiX9OsyX9fS9L2Lhnwdh4S/PXHb6i2rFVaJl5/BsRyQKdCFXRMRHlPRFRHxESV9ExEdq/Zh+dW7hCsVPt3WJiP/oTD8CnnzySQoKCkrmBw4cyJEjR6q93TVr1jBo0KBKrZObm8vChQsrva8jR47w9NNPV3o9EYktSvoRcHbSX7FiBfHx8VGJpaykX1bdfCV9kbpBSb+KnnjiCRITE0lMTOTJJ58kNzeXdu3aMWrUKNq3b8/NN99MQUEBs2fP5ssvv6RPnz706dMHgISEBL7++uuSdTIyMrjiiisYNWoUq1atIj09nbZt27Jx40aAkLXsK+L9998nNTWV1NRU0tLSOHbsGJMnT2bdunWkpqYya9YsXnjhBQYPHsy1115L3759OX78OH379qVTp04kJSWVlJKYPHkye/bsITU1lQcffBCAxx9/nK5du5KcnMwjjzxSst9f/epXXHnllfTq1YtbbrmFmTNnsmfPHjr1GVzSZ9ee3DPmRSQyav2YfjRs2rSJ+fPns2HDBpxzdO/enR//+Mfs3LmTuXPnkp6ezrhx43j66ad54IEHeOKJJ3jvvfe46KKLztnW7t27efXVV5k3bx5du3Zl4cKFrF+/nmXLljFjxgxee+012rVrF7KWfXlmzpzJnDlzSE9P5/jx4zRp0oTHHnuMmTNnsnz5ciBQPG3z5s1s2bKF5s2bc/r0aZYuXcoFF1zA119/zVVXXcXgwYN57LHHyM7OLqnlv3LlSnbt2sXGjRtxzjF48GDWrl3LD37wA5YsWcKnn35KYWEhnTp1onPnzlx++eVceEFTsrbmkJrUgfmL/sTYW28O6+siIuUr90zfzFqb2XtmlmNm28xsktfe3MzeNbNd3s9mXruZ2Wwz221mW8ysU9C2xnj9d5nZmJp7WjVr/fr1DBs2jPPPP5+4uDh+8pOfsG7dOlq3bk16ejoAt912G+vXry93W23atCEpKYl69erRsWNH+vbti5mRlJREbm4uEKhwOXz4cBITE7nvvvvYtm1bheJMT0/n/vvvZ/bs2Rw5coQGDUL/jb/++utLKnI655g6dSrJyclcd9115Ofnc+DAud9OtXLlSlauXElaWhqdOnVix44d7Nq1iw8++IAhQ4bQpEkTmjZtyo033liyzh23jWD+oiUUFRXx8mtvcutNN56zXRGpWRUZ3jkN/JtzrgNwFTDBzDoAk4HVzrm2wGpvHuAGoK33GA88A4E/EsAjQHegG/BI8R+KusLMypwPJbguf7169Urm69WrVzLGXtFa9mebPHkyf/jDHzhx4gTp6eklX994tuCqny+99BIHDx5k06ZNZGVlcemll4bcn3OOKVOmlNTN3717N7ffXna545tuHMBbq95n+Tt/oXNKokoqiERBucM7zrn9wH5v+piZbQdaAkOAa7xuLwJrgH/32he4QPnOj80s3swu8/q+65w7DGBm7wIDgEXVeQLRuMWyd+/eZGRkMHnyZJxzLF26lD/+8Y9MmjSJjz76iB49erBw4UJ69eoFfF9zPtTwTkWUVsu+PHv27CEpKYmkpCQ++eQTduzYQevWrc+ofx9qX5dccgkNGzbkvffe4/PPPz/jORTr378/06ZNY9SoUcTFxZGfn0/Dhg1JT0/n5z//OVOmTOH06dMsX76c8ePHA9CkSWP6X9ubux98hLm//X9VOBIiUl2VupBrZglAGrABuNT7gwDwd+BSb7olsC9otTyvrbT2s/cx3swyzSzz4MGDlQkvYjp16kRGRgbdunWje/fu3HHHHTRr1owrr7ySOXPm0L59e/7xj39w992BzxCMHz+eAQMGlFzIrazSatmX58knnyQxMZHk5GQaNmzIDTfcQHJyMvXr1yclJYVZs2ads86oUaPIzMwkKSmJBQsW0K5dOwBatGhBeno6iYmJPPjgg/Tr149bb72VHj16kJSUxM0338yxY8fo2rUrgwcPJjk5mRtuuIGkpCQuvPD7Gvmjbh5MvXpGvz69qnQsRKR6KlxP38zigPeB6c65P5vZEedcfNDyfzjnmpnZcuAx59x6r301gf8ArgGaOOf+02ufBpxwzoX+tg9qVz393NxcBg0aRHZ2drRDibrjx48TFxdHQUEBV199Nc899xydOnWCQ3uY+dQfOHrsGL+acl+NxrB9bz7tPzuz4NzEKBdcUz396lM9/Yqpdj19M2sILAFecs792Ws+YGaXOef2e8M3X3nt+UDroNVbeW35fD8cVNy+pqJPQmqP8ePHk5OTw8mTJxkzZkwg4QPDRt/Nntwv+MvSP0Y5QhH/KjfpW+Bq5Fxgu3PuiaBFy4AxwGPez9eD2iea2WICF22Pen8Y3gFmBF287QdMCc/TiL6EhISoneXPnz+f3/72t2e0paenR620cmkf/lq6wD9nWiKxqiJn+unAz4CtZpbltU0lkOxfMbPbgc+BEd6yFcBAYDdQAIwFcM4dNrNfAZ94/R4tvqhbWc65Ct0Z4xdjx46NyhejxCrnHMTw14CKRFNF7t5ZD5SWYfuG6O+ACaVsax4wrzIBnq1JkyYcOnSIFi1aKPHLOZxzHDp2kianYvMmAJFoq3WfyG3VqhV5eXnE6p09UoZ/RuA1c44mpw7SKv/Nmt+XSC1U65J+w4YNadOmTbTDkKo4++sLRSTiVHBNRMRHlPRFRHxESV9ExEeU9EVEfKTWXciVGrLwp9GOQEQiQElfRCKuOjV0pHo0vCMi4iNK+iIiPqKkLyLiI0r6IiI+oqQvIuIjSvoiIj6ipC8i4iNK+iIiPqKkLyLiI0r6IiI+ojIMIsDRE4UV7pu770jNBSJSw3SmLyLiI0r6IiI+ouEdiQkTT38R7RCkjqtOZc/Wzz4TxkiiS2f6IiI+oqQvIuIjSvoiIj6ipC8i4iO6kCtSS33GbyOynyuYdE7b7S98Uq1tDqvAZx1SWsdXax8Sms70RUR8RElfRMRHyk36ZjbPzL4ys+ygtl+aWb6ZZXmPgUHLppjZbjPbaWb9g9oHeG27zWxy+J+KiIiUpyJj+i8ATwELzmqf5ZybGdxgZh2AkUBH4F+AVWZ2hbd4DnA9kAd8YmbLnHM51YhdRKJo2KtPRDsEqYJyk75zbq2ZJVRwe0OAxc65U8BeM9sNdPOW7XbO/Q3AzBZ7fZX0RUQiqDpj+hPNbIs3/NPMa2sJ7Avqk+e1ldZ+DjMbb2aZZpZ58ODBaoQnIiJnq2rSfwa4HEgF9gO/CVdAzrnnnHNdnHNdLr744nBtVkREqOJ9+s65A8XTZvY8sNybzQdaB3Vt5bVRRruIiERIlc70zeyyoNlhQPGdPcuAkWbW2MzaAG2BjcAnQFsza2NmjQhc7F1W9bBFRKQqyj3TN7NFwDXARWaWBzwCXGNmqYADcoGfAzjntpnZKwQu0J4GJjjnirztTATeAeoD85xz28L9ZEREpGwVuXvnlhDNc8voPx2YHqJ9BbCiUtGJiEhY6RO5IiI+ooJrtcHCn0Y7AhGpI3SmLyLiI0r6IiI+oqQvIuIjSvoiIj6ipC8i4iNK+iIiPqKkLyLiI0r6IiI+oqQvIuIjSvoiIj6ipC8i4iNK+iIiPqKkLyLiI0r6IiI+oqQvIuIjSvoiIj6ipC8i4iNK+iIiPqKkLyLiI0r6IiI+oqQvIuIjSvoiIj7SINoBiNQ2Cad21Pg+chu3q/F9iD8p6YuIlGPfXXdXab3Wzz4T5kiqT8M7IiI+oqQvIuIjSvoiIj5SbtI3s3lm9pWZZQe1NTezd81sl/ezmdduZjbbzHab2RYz6xS0zhiv/y4zG1MzT0dERMpSkTP9F4ABZ7VNBlY759oCq715gBuAtt5jPPAMBP5IAI8A3YFuwCPFfyhERCRyyk36zrm1wOGzmocAL3rTLwJDg9oXuICPgXgzuwzoD7zrnDvsnPsH8C7n/iEREZEaVtUx/Uudc/u96b8Dl3rTLYF9Qf3yvLbS2s9hZuPNLNPMMg8ePFjF8EREJJRqX8h1zjnAhSGW4u0955zr4pzrcvHFF4drsyIiQtWT/gFv2Abv51deez7QOqhfK6+ttHYREYmgqib9ZUDxHThjgNeD2kd7d/FcBRz1hoHeAfqZWTPvAm4/r01ERCKo3DIMZrYIuAa4yMzyCNyF8xjwipndDnwOjPC6rwAGAruBAmAsgHPusJn9CvjE6/eoc+7si8MiIlLDyk36zrlbSlnUN0RfB0woZTvzgHmVik4EOHqiMNohiNQZ+kSuiIiPKOmLiPiIkr6IiI8o6YuI+IiSvoiIjyjpi4j4iJK+iIiPKOmLiPiIkr6IiI8o6YuI+IiSvoiIjyjpi4j4iJK+iIiPKOmLiPiIkr6IiI8o6YuI+IiSvoiIj5T7zVkiUndNeHV3uX3ieCICkUik6ExfRMRHlPRFRHxESV9ExEeU9EVEfERJX0TER5T0RUR8RElfRMRHlPRFRHxESV9ExEf0iVwRKdNx9kZkP3G0ich+/E5JX8o08fQX0Q5BRMKoWsM7ZpZrZlvNLMvMMr225mb2rpnt8n4289rNzGab2W4z22JmncLxBEREpOLCMabfxzmX6pzr4s1PBlY759oCq715gBuAtt5jPPBMGPYtIiKVUBMXcocAL3rTLwJDg9oXuICPgXgzu6wG9i8iIqWobtJ3wEoz22Rm4722S51z+73pvwOXetMtgX1B6+Z5bWcws/FmlmlmmQcPHqxmeCIiEqy6F3J7OefyzewS4F0z2xG80DnnzMxVZoPOueeA5wC6dOlSqXVFRKRs1TrTd87lez+/ApYC3YADxcM23s+vvO75QOug1Vt5bSIiEiFVTvpmdr6ZNS2eBvoB2cAyYIzXbQzwuje9DBjt3cVzFXA0aBhIREQioDrDO5cCS82seDsLnXNvm9knwCtmdjvwOTDC678CGAjsBgqAsdXYt0idlnBqR/mdqim3cbsa34ff7bvr7iqv2/rZmrnBscpJ3zn3NyAlRPshoG+IdgdMqOr+RESk+lR7R0TER5T0RUR8RLV3pMqOniiMdggiUklK+tW18KfRjkBEpMI0vCMi4iNK+iIiPqKkLyLiI0r6IiI+oqQvIuIjuntHpA4Y/trJSq9TUG93DUQisU5n+iIiPqKkLyLiI0r6IiI+ojF9EYlJn+47UuP7SGkdX+P7iDU60xcR8RElfRERH1HSFxHxESV9EREf0YXcOiorTBfBjsarZr5IXVK3k75q3YuInEHDOyIiPqKkLyLiI3V7eEdEao3j7I3IfuJoE5H9xCqd6YuI+IiSvoiIj2h4p5aaePqLMpfrVkspz3nfHa/xfRTUi6vxfUjl6ExfRMRHdKYvEiOq8u1XIpWlpB8F4fi0rIZvRKQqIj68Y2YDzGynme02s8mR3r+IiJ9FNOmbWX1gDnAD0AG4xcw6RDIGERE/i/TwTjdgt3PubwBmthgYAuREOI5SVXfo5Tfxh8rvFF+tXUiM09j892LxDqHgD4Ft/bphuMMpkXRRYo1tuzoinfRbAvuC5vOA7sEdzGw8MN6bPW5mOyMUW7GLgK8jvM/KiPX4wOcx/nd4NuPrYxhGUYxxfUU7ho7x989WZ+f/q7QFMXch1zn3HPBctPZvZpnOuS7R2n95Yj0+UIzhEOvxgWIMl0jHGOkLuflA66D5Vl6biIhEQKST/idAWzNrY2aNgJHAsgjHICLiWxEd3nHOnTazicA7QH1gnnNuWyRjqICoDS1VUKzHB4oxHGI9PlCM4RLRGM05F8n9iYhIFKn2joiIjyjpi4j4iJI+YGaPm9kOM9tiZkvNLN5rTzCzE2aW5T2qdeNsGOKMqRIWZtbazN4zsxwz22Zmk7z2X5pZftBxGxjlOHPNbKsXS6bX1tzM3jWzXd7PZlGM78qgY5VlZt+Y2b3RPo5mNs/MvjKz7KC2kMfNAmZ7v5tbzKxTlOKLqfdyKTGW+rqa2RTvGO40s/41EpRzzvcPoB/QwJv+NfBrbzoByI52fF4s9YE9wA+BRsCnQIcox3QZ0Mmbbgp8RqC8xi+BB6J9zILizAUuOqvtv4DJ3vTk4tc82g/vdf47gQ/XRPU4AlcDnYLfA6UdN2Ag8BZgwFXAhijFF1Pv5VJiDPm6eu+dT4HGQBvv/V4/3DHpTB9wzq10zp32Zj8m8PmBWFNSwsI59y1QXMIiapxz+51zm73pY8B2Ap+6rg2GAC960y8CQ6MXyhn6Anucc59HOxDn3Frg8FnNpR23IcACF/AxEG9ml0U6vlh7L5dyDEszBFjsnDvlnNsL7Cbwvg8rJf1zjSNwxlKsjZn91czeN7Pe0QqK0CUsYibBmlkCkAZs8Jomev9iz4vm0InHASvNbJNX5gPgUufcfm/678Cl0QntHCOBRUHzsXQcofTjFou/n7H6XobQr2tEjqFvkr6ZrTKz7BCPIUF9HgZOAy95TfuB/+mcSwPuBxaa2QWRjz62mVkcsAS41zn3DfAMcDmQSuAY/iZ60QHQyznXiUB11wlmdnXwQhf43zrq9y57H1gcDLzqNcXacTxDrBy3UGL8vRzV1zXmau/UFOfcdWUtN7MMYBDQ1/tlxjl3CjjlTW8ysz3AFUBmzUYbUkyWsDCzhgQS/kvOuT8DOOcOBC1/HlgepfAAcM7lez+/MrOlBP5lPmBmlznn9nvDEF9FM0bPDcDm4uMXa8fRU9pxi5nfz1h/L5fxukbkGPrmTL8sZjYAeAgY7JwrCGq/2ALfAYCZ/RBoC/wtOlHGXgkLMzNgLrDdOfdEUHvwWO4wIPvsdSPFzM43s6bF0wQu9GUTOHZjvG5jgNejE+EZbiFoaCeWjmOQ0o7bMmC0dxfPVcDRoGGgiKkN7+UyXtdlwEgza2xmbQjEuDHsAUT6anYsPghcMNkHZHmPZ732m4BtXttm4MYoxzmQwB0ye4CHY+C49SLw7/2WoGM3EPgjsNVrXwZcFsUYf0jgjohPvdfyYa+9BbAa2AWsAppH+VieDxwCLgxqi+pxJPAHaD9QSGB8+fbSjhuBu3bmeL+bW4EuUYovpt7LpcRY6usKPOwdw53ADTURk8owiIj4iIZ3RER8RElfRMRHlPRFRHxESV9ExEeU9EVEfERJX0TER5T0RUR85P8D6CIKuecNXrEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for strategy in strategies: \n",
    "    plt.hist(all_results[strategy.__name__].values(), label =strategy.__name__, alpha = 0.7)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.5032\n",
      "35.4256\n",
      "64.5967\n",
      "118.8758\n"
     ]
    }
   ],
   "source": [
    "for strategy in strategies: \n",
    "    print(np.mean(list(all_results[strategy.__name__].values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADP algorithm finding the best strategy \n",
    "$$V_t(s, \\theta) = \\theta_0 + \\theta_{t,1} \\cdot \\phi_1(s) + \\theta_{t,2} \\cdot \\phi_2(s) + \\theta_{t,3} \\cdot \\phi_3(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\phi_i(s)$ functions are onli indicator functions of each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADP steps: \n",
    "1. Initialize parameters $\\theta_t$\n",
    "2. Define basis functions $\\phi_i(s)$\n",
    "3. Loop over time epochs from the last to the first: \n",
    "    1. Loop over fixed amount of sampled states: \n",
    "        1. Determine optimal decisions based on those states, in my case 1 action out of 6. \n",
    "        2. Simulate what happens after my action (s,a,s) triplet and reward (plus value function estimate in the following state) \n",
    "    2. Make a linear regression on the new findings. Get new $\\theta_i$ based on rewards + future vf and values of the basis functions. \n",
    "    3. Update the parameters, either fully, or with a learning step. \n",
    "4. Check how far are the value functions from the truth (which I know here) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. initialize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = [[0 for i in range(3)] for i in time_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_initial = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_1(s): \n",
    "    return 1 if s == 0 else 0 \n",
    "\n",
    "def bf_2(s): \n",
    "    return 1 if s == 1 else 0 \n",
    "\n",
    "def bf_3(s): \n",
    "    return 1 if s == 2 else 0 \n",
    "basis_functions = [bf_1,bf_2,bf_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a determine optimal decision based on a state and simulate what happens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_mult(a,b): \n",
    "    return sum([i*j for i,j in zip(a,b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 2\n",
    "action = 1\n",
    "actions = range(2)\n",
    "states = range(3)\n",
    "t = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3, 0.4, 0.3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_matrix[2][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 18, 14]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem_setup.reward_matrix[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.399999999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_mult(prob_matrix[2][1], reward_matrix[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_example.vf import Vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vfs(time_epochs, theta_initial):\n",
    "    vfs = []\n",
    "    for i in time_epochs: \n",
    "        vfs.append(Vf(theta_initial))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_example.setup import ProblemSetup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_value(state, action, prob_matrix, reward_matrix, future_vf): \n",
    "    rewards = reward_matrix[state][action]\n",
    "    future_vf_values = future_vf.compute_all_values()\n",
    "    total_rewards = [i+j for i,j in zip(rewards, future_vf_values)]\n",
    "    return vector_mult(prob_matrix[state][action], total_rewards) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(state, future_vf, prob_matrix, reward_matrix): \n",
    "    exp_rewards = []\n",
    "    for action in actions: \n",
    "        exp_rewards.append(get_exp_value(state, action, prob_matrix, reward_matrix, future_vf))\n",
    "    return np.argmax(exp_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b for x random states, determine the best action, perform the evolution, get the actual reward and note it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_reward_pairs(sample_size, states, future_vf, prob_matrix, reward_matrix): \n",
    "    state_reward_pairs = []\n",
    "    for i in range(sample_size): \n",
    "        state = np.random.choice(states, p=[0.34, 0.33, 0.33])\n",
    "\n",
    "        action = get_best_action(state, future_vf, prob_matrix, reward_matrix) \n",
    "        new_state = get_new_state(state, action, prob_matrix)\n",
    "\n",
    "        reward = reward_matrix[state][action][new_state] + future_vf.compute_value(new_state)\n",
    "        state_reward_pairs.append([state, reward])\n",
    "    return state_reward_pairs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c linear regression on the results \n",
    "- I have the following model: \n",
    "$$V_t(s, \\theta) = \\theta_0 + \\theta_{t,1} \\cdot \\phi_1(s) + \\theta_{t,2} \\cdot \\phi_2(s) + \\theta_{t,3} \\cdot \\phi_3(s)$$\n",
    "- and I want to predict the parameters \\theta, based on the realizaitons V_t(s) and the variable s. \n",
    "- This model is simple, and we expect that each of the parameters will be close to the expected value. \n",
    "    - In reality, it will be such a number that minimizes the sum of squares of the errors. \n",
    "    - For example if there are three outcomes with 1/3 probability and rewards 1,3,100, the number will be closer to 50 than 33. \n",
    "    - Thus even for this model, the result will in uneven settings of reeward be only an approximation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.c Intercept makes the model crazy \n",
    "- There are numbers like 10^14 \n",
    "- Since there is no reason for the intercept, i will just make a model without it here. The model becomes: \n",
    "\n",
    "$$V_t(s, \\theta) = \\theta_{t,1} \\cdot \\phi_1(s) + \\theta_{t,2} \\cdot \\phi_2(s) + \\theta_{t,3} \\cdot \\phi_3(s)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing the regression variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_regression_variables(state_reward_pairs, basis_functions): \n",
    "    x = []\n",
    "    y = []\n",
    "    for pair in state_reward_pairs: \n",
    "        x.append([basis_functions[0](pair[0]),\n",
    "                  basis_functions[1](pair[0]),\n",
    "                  basis_functions[2](pair[0])])\n",
    "        y.append(pair[1])\n",
    "        \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actual values = 6.4, 13.8, 14.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But the actual prediction makes sense i guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vf_coef(current_vf, next_vf, problem_setup, sample_size, basis_functions):\n",
    "    state_reward_pairs_raw = get_state_reward_pairs(sample_size, \n",
    "                                                    problem_setup.states, \n",
    "                                                    next_vf,\n",
    "                                                    problem_setup.prob_matrix,\n",
    "                                                    problem_setup.reward_matrix)\n",
    "    \n",
    "    x,y = prepare_regression_variables(state_reward_pairs_raw, basis_functions)\n",
    "    model = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    current_vf.set_params(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "106.58909091199999, 113.321546176, 113.26535516799999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 5000 samples and 20 loops of update, there is still a 0.5% difference from the reality. This might be caused by a different optimization function as discussed above. Linear algorithm does not return the mean value, rather a value from which the sum of squares of residuals is the lowest. \n",
    "\n",
    "A reasonable approximation can be seen as low as for 50 samples and 10 loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_setup = ProblemSetup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_example.adp_algorithm import adp_algorithm_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (20 of 20) |########################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "vfs_1 = adp_algorithm_final(loops_of_update=20, sample_size=50, problem_setup = problem_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104.17630451, 113.07600179, 110.85370641])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfs_1[0].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "106.58909091199999, 113.321546176, 113.26535516799999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:02:43 Time:  0:02:43\n"
     ]
    }
   ],
   "source": [
    "vfs_1 = adp_algorithm_final(loops_of_update=100, sample_size=2000, problem_setup = problem_setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106.69024172, 113.28469842, 113.22032903])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vfs_1[0].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "106.58909091199999, 113.321546176, 113.26535516799999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "- I have implemented the ADP algorithm on the simple example. \n",
    "- We can see that the approximation is really good in this example. The implementation is working. \n",
    "- This investigation will help with the implementation of the actual problem in the next phase. \n",
    "    - The complexity will rise significantly, each of the steps will be somehow harder and the computational complexity will rise too. Nevertheless, the framework is ready to be used. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
